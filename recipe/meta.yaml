{% set name = "inference-server" %}
{% set version = "1.3.2" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/inference_server-{{ version }}.tar.gz
  sha256: 54ff9296d5babcde559480a176fc02418727b909ab2b940a2fc8ef1b40daf00c

build:
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0

requirements:
  host:
    - python >=3.8
    - setuptools >=45
    - wheel
    - setuptools-scm >=6.2
    - pip
  run:
    - python >=3.8
    - botocore
    - codetiming >=1.4,<2.dev0
    - orjson >=3.0,<4.dev0
    - pluggy >=1.0,<2.dev0
    - werkzeug >=2.0,<3.dev0

test:
  imports:
    - inference_server
  commands:
    - pip check
  requires:
    - pip

about:
  summary: Deploy your AI/ML model to Amazon SageMaker for Real-Time Inference and Batch Transform using your own Docker container image.
  description: |
    inference-server is a Python library to deploy an AI/ML model to Amazon SageMaker for real-time inference. 
    The library simplifies deploying a model using your own Docker container image.
  home: https://github.com/jpmorganchase/inference-server
  dev_url: https://github.com/jpmorganchase/inference-server
  doc_url: https://inference-server.readthedocs.io
  doc_source_url: https://github.com/jpmorganchase/inference-server/blob/main/docs/index.rst
  license: Apache-2.0
  license_file: LICENSE

extra:
  recipe-maintainers:
    - faph
